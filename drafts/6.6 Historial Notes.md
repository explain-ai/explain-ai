# Historical Notes on Deep Feedforward Networks

So by now it is clear that we use feedforward networks as approximations to nonlinear functions.
And we use gradient descent to minimize the error.
<aside>
There was a question in CS230 - Stanford, "The universal approximation theorem states that a neural network with a single hidden layer can approximate any continuous function (with some assumptions on the activation). Give one reason why you would use deep networks with multiple layers."^[https://cs230.stanford.edu/files/cs230exam_spr18_soln.pdf]
</aside>

* Chain Rule - (Leibniz, 1676; L'Hopital,1696)
* Gradient Descent (Cauchy ,1847)

In the 1940's above techniques were used to approximate linear functions (Perceptron) - This couldn't learn XOR.

* To approximate Non-linear functions it was needed to make a multilayer perceptron and most importantly to compute the gradient. 
First successful experiments with backpropagation was introduced in the book [Parallel Distributed Processing](https://mitpress.mit.edu/books/parallel-distributed-processing-volume-1)[@rumelhart_mcclelland_1986]